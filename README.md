# Extracting and processing Wikipedia articles
Extract only text from JSON output of [WikiExtractor](https://github.com/attardi/wikiextractor).
By default, the title of the article is removed, the output is segmented (one sentence per line) 
and tokenized (tokens are space-separated).

## Usage

```
usage: extract-articles.py [-h] [--keep-headings] [--max-tokens MAX_TOKENS]
                           [--min-tokens MIN_TOKENS] [-n N_JOBS]
                           [--no-segmentation] [--no-tokenized-output] [--raw]
                           [--spacy-model SPACY_MODEL]
                           din dout

Process files generated by WikiExtractor and create one file per Wikipedia
article. Articles are grouped per the initial(s) of their title.

positional arguments:
  din                   input directory. All files in all subdirectories will
                        be processed.
  dout                  output directory.

optional arguments:
  -h, --help            show this help message and exit
  --keep-headings       do not remove the first line (article heading) of an
                        article. (default: False)
  --max-tokens MAX_TOKENS
                        sentences with more than 'max_tokens' won't be
                        included in the output. (default: None)
  --min-tokens MIN_TOKENS
                        sentences with less than 'min_tokens' won't be
                        included in the output. (default: None)
  -n N_JOBS, --n-jobs N_JOBS
                        number of workers to use (default depends on your
                        current system; core count - 1). (default: 3)
  --no-segmentation     by default, the output will write one sentence per
                        line. This option prevents such line segmentation.
                        (default: False)
  --no-tokenized-output
                        do not tokenize the articles. (default: False)
  --raw                 store the articles as-is. This is identical to setting
                        'keep-headings' and 'no-segmentation' both to True.
                        (default: False)
  --spacy-model SPACY_MODEL
                        spaCy model to use for sentence segmentation.
                        (default: en_core_web_sm)
```

## Requirements (cf. Pipfile)
 - Python 3.6 or higher
 - [spaCy](https://spacy.io/usage/)
 - a spaCy [language model](https://spacy.io/usage/models) (en_core_web_sm by default)
 - [python-slugify](https://github.com/un33k/python-slugify)
 
For the lazy:

```bash
git clone https://github.com/BramVanroy/wiki-processing.git
cd wiki-processing
pipenv install
pipenv run python -m spacy download en_core_web_sm
```

## Important note
Because of how I implemented the sentence boundary detection through spaCy, consecutive direct speech clauses are not segmented. 
What this means is that a sentence such as 

> Wilson said of the "Times"' owner, the Journal Register Company, "They don't care about the product. They don't care about the customer. They don't care about the employees. And they don't know anything about the business."

will be parsed as one single sentence.

For more information, see [this issue #3553](https://github.com/explosion/spaCy/issues/3553).
