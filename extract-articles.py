from concurrent.futures import ProcessPoolExecutor
import json
from os import cpu_count
from pathlib import Path
import time

from slugify import slugify
"""
    Processes the JSON output of WikiExtractor: creates one file per Wikipedia article.
    Filenames are unique and based on the ID and title of the article.
    No preprocessing is done.
"""


DEFAULT_WORKERS = cpu_count() - 1 or 1


class ArticleExtractor:
    def __init__(self, keep_headings=False, n_jobs=DEFAULT_WORKERS, no_segmentation=False):
        self.keep_headings = keep_headings
        self.n_jobs = n_jobs
        self.no_segmentation = no_segmentation

        self.pdin = None
        self.pdout = None

    def extract_articles(self, din, dout):
        """
        Iterate over all subdirectories and process all files with 'process_file'
        """
        self.pdin = Path(din).resolve()
        self.pdout = Path(dout).resolve()

        start_time = time.time()

        total_articles_n = 0
        files = (pfin for pfin in self.pdin.rglob('*') if pfin.is_file())

        with ProcessPoolExecutor(max_workers=self.n_jobs) as executor:
            print(f"Processing dir {str(self.pdin)} with {self.n_jobs} threads...")
            for filename, article_n in executor.map(self.process_file, files):
                total_articles_n += article_n
                print(f"\rWrote {article_n} articles from file {filename}...", end='', flush=True)

        print(f"Finished! Wrote {total_articles_n} articles in {time.time() - start_time:.0F} seconds.")

    def parse_json(self, line):
        """
        Parses JSON from line.
        Uses the 'id' and 'title' fields to generate a unique filename.
        Writes 'text' field to the new filename.
        """
        obj = json.loads(line)

        slug = slugify(obj['title'], max_length=36)
        filename = f"{slug}-{obj['id']}.txt" if slug else f"{obj['id']}.txt"
        initial_dir = self.pdout.joinpath(ArticleExtractor.get_initials(slug))
        initial_dir.mkdir(exist_ok=True)
        filename = initial_dir.joinpath(filename)

        with open(filename, 'w', encoding='utf-8') as fhout:
            fhout.write(obj['text'])

    def process_file(self, pfin):
        """
        Process all lines in a file with 'parse_json'.
        One JSON object per line.
        """
        article_n = 0
        with open(pfin, 'r', encoding='utf-8') as fhin:
            for line in fhin:
                line = line.strip()

                if line == '':
                    continue
                article_n += 1
                self.parse_json(line)

        return pfin.name, article_n

    @staticmethod
    def get_char_at_idx(filename, idx):
        """ Get character at index 'idx', return None for IndexError. """
        try:
            c = filename[idx].lower()
        except IndexError:
            c = None

        return c

    @staticmethod
    def get_initials(filename):
        """
        Get the first two characters of a filename.
        Fallback to first character if second is not alnum
        Fallback to 'other' if first is not alnum
        Fallback to 'other' if filename is false-y, i.e. ''
        """
        if not filename:
            return 'other'

        first = ArticleExtractor.get_char_at_idx(filename, 0)

        if first and first.isalnum():
            second = ArticleExtractor.get_char_at_idx(filename, 1)

            if second and second.isalnum():
                return first + second
            else:
                return first
        else:
            return 'other'


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Process files generated by WikiExtractor and create one file per'
                                                 ' Wikipedia article. Articles are grouped per the initial of'
                                                 ' their title.')
    parser.add_argument('din', help='input directory. All files in all subdirectories will be processed.')
    parser.add_argument('dout', help='output directory.')
    parser.add_argument('-n', '--n_jobs', type=int, default=DEFAULT_WORKERS,
                        help=f"number of threads to use (default: {DEFAULT_WORKERS}).")
    parser.add_argument('--raw', action='store_true', default=False,
                        help="store the articles as-is. This is identical to setting 'keep-headings' and"
                             " 'no-segmentation' both to True.")
    parser.add_argument('--keep-headings', action='store_true', default=False,
                        help='do not remove the first line (article heading) of an article.')
    parser.add_argument('--no-segmentation', action='store_true', default=False,
                        help='by default, the output will print one sentence per line. This option prevents such'
                             ' line segmentation.')

    args = parser.parse_args()

    if args.raw:
        args.keep_headings = True
        args.no_segmentation = True
    del args.raw

    extractor = ArticleExtractor(args.keep_headings, args.n_jobs, args.no_segmentation)
    extractor.extract_articles(args.din, args.dout)
