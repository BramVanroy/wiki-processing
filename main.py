from concurrent import futures
import json
from pathlib import Path
import time

from slugify import slugify
"""
    Processes the JSON output of WikiExtractor: creates one file per Wikipedia article.
    Filenames are unique and based on the ID and title of the article.
    No preprocessing is done.
"""


def get_char_at_idx(filename, idx):
    """ Get character at index 'idx', return None for IndexError. """
    try:
        c = filename[idx].lower()
    except IndexError:
        c = None

    return c


def get_initials(filename):
    """
    Get the first two letters of a filename.
    Fallback to first letter if second is not alnum
    Fallack to 'other' if first is not alnum
    """
    first = get_char_at_idx(filename, 0)

    if first and first.isalnum():
        second = get_char_at_idx(filename, 1)

        if second and second.isalnum():
            return first + second
        else:
            return first
    else:
        return 'other'


def parse_json(line, pdout):
    """
    Parses JSON from line.
    Uses the 'id' and 'title' fields to generate a unique filename.
    Writes 'text' field to the new filename.
    """
    obj = json.loads(line)

    slug = slugify(obj['title'], max_length=36)
    filename = f"{slug}-{obj['id']}.txt" if slug else f"{obj['id']}.txt"
    initial_dir = pdout.joinpath(get_initials(slug))
    initial_dir.mkdir(exist_ok=True)
    filename = initial_dir.joinpath(filename)

    with open(filename, 'w', encoding='utf-8') as fhout:
        fhout.write(obj['text'])


def process_file(pfin, pdout):
    article_n = 0
    with open(pfin, 'r', encoding='utf-8') as fhin:
        for line in fhin:
            line = line.strip()

            if line == '':
                continue
            article_n += 1
            parse_json(line, pdout)

    return pfin.name, article_n


def _multi_main(pdin, pdout, njobs):
    total_articles_n = 0
    files = (pfin for pfin in pdin.rglob('*') if pfin.is_file())

    with futures.ThreadPoolExecutor(max_workers=njobs) as executor:
        print(f"Processing dir {str(pdin)} with {default_jobs} threads...")
        for filename, article_n in executor.map(lambda f: process_file(f, pdout), files):
            total_articles_n += article_n
            print(f"Wrote {article_n} articles from file {filename}...")

    return total_articles_n


def _single_main(pdin, pdout):
    print(f"Processing dir {str(pdin)} single-threaded...")
    total_articles_n = 0
    files = (pfin for pfin in pdin.rglob('*') if pfin.is_file())

    for pfin in files:
        filename, article_n = process_file(pfin, pdout)
        total_articles_n += article_n
        print(f"Wrote {article_n} articles from file {filename}...")

    return total_articles_n


def main(pdin, pdout, njobs):
    """
    Iterate over all subdirectories and process all files with 'parse_json'
    """
    start_time = time.time()

    if njobs == 1:
        total_articles_n = _single_main(pdin, pdout)
    else:
        total_articles_n = _multi_main(pdin, pdout, njobs)

    print(f"Finished! Wrote {total_articles_n} articles in {time.time() - start_time:.0F} seconds.")


if __name__ == '__main__':
    import argparse
    import os

    default_jobs = (os.cpu_count() or 1) * 5
    parser = argparse.ArgumentParser(description='Process files generated by WikiExtractor and create one file per'
                                                 ' Wikipedia article. Articles are grouped per the initial of'
                                                 ' their title.')
    parser.add_argument('din', help='input directory. All files in all subdirectories will be processed.')
    parser.add_argument('dout', help='output directory.')
    parser.add_argument('-n', '--njobs', type=int, default=default_jobs,  help=f"number of threads to use"
                                                                               f" (default: {default_jobs}).")

    args = parser.parse_args()
    main(Path(args.din).resolve(),
         Path(args.dout).resolve(), njobs=args.njobs)
